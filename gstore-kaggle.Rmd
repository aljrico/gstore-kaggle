---
title: "gstore-eda"
author: "Alejandro Jiménez Rico"
date: "28 September 2018"
output:
 html_document:
    fig_width: 10
    fig_height: 7
    toc: yes
    number_sections : no
    code_folding: show
---


In today's competition we a’re challenged to analyze a Google Merchandise Store (also known as GStore, where Google swag is sold) customer dataset to predict revenue per customer.

Finally, we have a competition more R-oriented. Our moment to shine. 

In this notebook, I'll try to explore the given dataset and make some inferences along the way in order to build a baseline model to get started.


```{r}
library(tidyverse)
library(data.table)
library(jsonlite)
library(lubridate)
library(moments)

library(caret)
library(xgboost)

library(viridis)
library(harrypotter)

library(lime)
```


# Retrieving Data

As always, we have to start by taking a look at the actual data. 
```{r}
test  <- fread("data/test.csv")
test %>% glimpse()

rm(test)
gc()
```

Do you notice those weird patterns in the columns `device`, `geoNetwork`, `totals` and `trafficSource`? This isn't suppose to be a `.csv` file. It has tree-developed features that we need to flatten out.


```{r}

flatten <- function(x){
	
	pre_flatten <- . %>% 
  str_c(., collapse = ",") %>% 
  str_c("[", ., "]") %>% 
  fromJSON(flatten = T)	
	
	x %>% 
  bind_cols(pre_flatten(.$device)) %>%
  bind_cols(pre_flatten(.$geoNetwork)) %>% 
  bind_cols(pre_flatten(.$trafficSource)) %>% 
  bind_cols(pre_flatten(.$totals)) %>% 
  select(-device, -geoNetwork, -trafficSource, -totals)	%>% 
	return()
}


train <- read_csv("data/train.csv") %>% sample_n(9e4) %>% flatten() %>% data.table()
test  <- read_csv("data/test.csv")  %>% sample_n(9e4) %>%  flatten() %>% data.table()
```


```{r}
glimpse(train)
glimpse(test)
```

# Broad Missing Values Treatment

After flattening the columns, we can spot _a lot_ of hidden missing values. And I mean missing by those `not available in demo dataset` and similars. Make no mistake, those are missing values also and should be treated as that.

```{r}
hidden_na <- function(x) x %in% c("not available in demo dataset", 
																	"(not provided)",
                                  "(not set)", 
																	"<NA>", 
																	"unknown.unknown",  
																	"(none)",
																	"Not Socially Engaged")

train <- train %>%  mutate_all(funs(ifelse(hidden_na(.), NA, .))) %>% data.table()
test  <- test  %>%  mutate_all(funs(ifelse(hidden_na(.), NA, .))) %>% data.table()
```

```{r nas1, result='asis', echo=FALSE}
train %>% 
	summarise_all(funs(sum(is.na(.))/n()*100)) %>% 
	gather(key = "feature", value = "missing_pct") %>% 
	filter(missing_pct > 0) %>% 
  ggplot(aes(x = reorder(feature,missing_pct), 
  					 y = missing_pct
  					 )
  			 ) +
  geom_bar(stat = "identity", 
  				 colour = "black",
  				 fill = viridis(3)[[2]]) +
		geom_text(aes(label = paste0(round(missing_pct,1), " (%)"),
								y= missing_pct),
						hjust = -1) +
  labs(y = "Missing Values (%)", x = "Features") +
  coord_flip() +
  theme_minimal()
```


```{r}
train[,which(unlist(lapply(train, function(x)!all(is.na(x))))),with=F]
test[,which(unlist(lapply(test, function(x)!all(is.na(x))))),with=F]
```



# Defining the Target

In this competition, our task is - as awlays - to predict a target $T$. The target today is defined as the natural log of the sum of all transactions ($t$) per user ($u$). which can be written down as:

$$T_u = ln\left(\sum_{i = 1}^N t_u\right)$$

We can visualize its distribution (from those who have spent some money).

```{r}
train %>%
	select(fullVisitorId,transactionRevenue) %>% 
	na.omit() %>% 
	group_by(fullVisitorId) %>% 
	summarise(logRevenue = log(sum(as.numeric(transactionRevenue)))) %>% 
	ggplot(aes(x = logRevenue)) +
	geom_histogram(aes(y = 100*..count../sum(..count..)), colour = "black", fill = viridis(3)[[2]], bins = 50) +
	theme_minimal() +
	xlab("Target") +
	ylab("(%)")
```

It looks quite beatiful, but please note that we are - quite fairly - assuming that the missing values in the `transactionRevenue` column are simply an absence of transactions. Which is $0$ revenue.

```{r}
train[, transactionRevenue := ifelse(is.na(transactionRevenue), 0, transactionRevenue)]
```


> The Pareto principle - also known as the 80/20 rule - states that, for many events, roughly 80% of the effects come from 20% of the causes. And it is a consequent axiom of business management that _80% of sales come from 20% of clients_.

Reality is, most people we have recorded in the dataset don't end up buying things in the store. 

```{r}
train %>% 
	select(fullVisitorId,transactionRevenue) %>% 
	group_by(fullVisitorId) %>% 
	summarise(logRevenue = (sum(as.numeric(transactionRevenue)))) %>% 
	mutate(logRevenue = ifelse(logRevenue == 0, "No", "Yes")) %>% 
	ggplot(aes(x = logRevenue,
						 fill = logRevenue)) +
	geom_bar(aes(y = 100*..count../sum(..count..)), 
					 colour = "black"
					 ) +
	geom_text(aes(label = scales::percent(..count../sum(..count..)),
								y= 100*..count../sum(..count..) ), 
						stat= "count", 
						vjust = -.5) +
	theme_minimal() +
	xlab("Did they spend any money whatsoever?") +
	ylab("(%)") +
	labs(fill = "")
```

What these numbers are suggesting is that, before even considering predicting _how much_ money a customer is going to spend, we should begin by thinking _whether_ a customer is going to spend _any money_ whatsoever. 

This detail is crucial because it forces us to not start by constructing a _regression model_, but a _classification_ one.

## Time Dependence of the Target variable

```{r}
train %>% 
	group_by(fullVisitorId, date) %>% 
	summarise(did_she_spend = sum(ifelse(transactionRevenue == 0, 0, 1))) %>%
	group_by(date) %>% 
	summarise(payer_perc = 100*sum(did_she_spend)/n()) %>% 
	ggplot(aes(x = ymd(date), y = payer_perc)) +
	geom_line(colour = "black", size = 0.5) +
	geom_smooth(method = "loess",
							formula = y ~ x,
							level = 0.9,
							colour = viridis(3)[[2]], 
							size = 1.5) +
	ylab("(%) of Customers that bought something.") +
	xlab("Date")
```

What insights can we draw from this plot? First and foremost, we can not spot an obvious trend. The usual percentage bounces between $1\%$ and $2\%$, we might get suspicious if the result of our model outputs something away from that. It seems that the store was somehow inefficient at the end of 2016 and beginning of 2017, and it surely suffered major changes that improved conversion and the tendency dramatically. I'd wonder, however, what the hell happened in April 2017. That is impressive.

What I really find interesting are the periodicity of some of the bounces. If you take a closer look at the plot, you can see that it has periodic spikes. I'd say that those spikes have to do with the day of the week.

```{r}
train %>% 
	mutate(date = date %>% ymd()) %>% 
	group_by(fullVisitorId, date) %>% 
	summarise(did_she_spend = sum(ifelse(transactionRevenue == 0, 0, 1))) %>%
	mutate(date = date %>% wday(label = TRUE, abbr = FALSE)) %>% 
	group_by(date) %>% 
	summarise(payer_perc = 100*sum(did_she_spend)/n()) %>% 
	ggplot(aes(x = date,
						 y = payer_perc)) +
	geom_col(colour = "black",
					 fill = viridis(3)[[2]])
```

As I suspected, something happens during the weekends. My shot is that more "casual" people enter wandering at the store, just for browsing out of curiosity without any intention of buying anything in the first place. Yes, some people might end up buying something, but I expected that even though the gross number of customers increase on weekends, the percentage of them that buy something would drecrease.

This can be used as a feature. Since we should expect that a weekend customer is less likely to buy anything.

```{r}
train_class <- train %>%
	mutate(date = ymd(date)) %>% 
	group_by(fullVisitorId, date) %>% 
	mutate(target_class = sum(ifelse(transactionRevenue == 0, 0, 1))) %>%
	mutate(weekend = ifelse((date %>% wday(label = TRUE, abbr = FALSE)) %in% c("Saturday", "Sunday"),1,0)) %>% 
	data.table()
```

# Classification Model

```{r}
fn <- funs(mean, 
					 # sd, 
					 # min, 
					 # max, 
					 # sum, 
					 # n_distinct, 
					 # kurtosis, 
					 # skewness, 
					 .args = list(na.rm = TRUE))

```


```{r}
train_class <- train %>% 
	mutate(date = ymd(date)) %>% 
	group_by(fullVisitorId, date) %>% 
	mutate(target_class = sum(ifelse(transactionRevenue == 0, 0, 1))) %>% 
	mutate(target_class = ifelse(target_class > 1, 1, target_class)) %>% 
	as_tibble()

test_class <- test %>% 
	mutate(date = ymd(date)) %>% 
	group_by(fullVisitorId, date) %>% 
	as_tibble()
```

By now, we should have noticed a big problem that we are facing here. Note that we have far more instances for `target_class == 0` than for `target_class == 1`. This is what we call *unbalanced data*. This is a problem, because any model that we might build will tend to label every output as `0`, in order to increase their accuracy. And that is going to get the model more accuracte, indeed; but is not going to be useful for us.

Imagine that you are building an AI model in order to spot and diagnose illnesses in people. An extremely accurate model would be one that _always_ diagnoses no illness; a model that just claims that every person is healthy. That would be extremely accurate, wouldn't it? After all, most people are healthy the vast majority of the time. Dspite being accurate, such a model turns out to be utterly useless, because is not telling you anything useful.

Anagolously, our model will tend to spot every possible customer as `non-payer`. Making the model accurate, but pointless.

There are a myriad of techniques in order to avoid it. For simplicity's sake, we are going to use _oversampling_. This is a kind of [bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping) technique, because we simply duplicate some of our instances, in order to balance the data a little bit.


```{r}
n_pos <- sum(train_class$target_class==1)
n_tot <- nrow(train_class)
train_positive <- train_class %>% 
	filter(target_class == 1) %>% 
	sample_n(size = floor((n_tot - n_pos)/(n_pos)), replace = TRUE)

train_class <- rbind(train_positive, train_class) %>% as_tibble()
```

Now, we have our data splitted evenly. $50\%$ of our instances show `target_class == 0` and $50\%$ are `target_class == 1`.


## EDA

Note that these EDA is going to be made after resampling the data. That could seem utterly biased, and it is. The point is that, as long as we are aware of that, we'll be ok. The purpose of this EDA is not simply to explore the data as it is, but to extract relevant features. Resampling the data lets us put the features in perspective. As we will see followingly, features evenly distributed (50-50) amongs those who paid something and those who didn't, is telling us basically nothing that could help us predict the target.

### Weekends

The first variable that I'd like to explore is that one already spotted in the time dependence study of our data. A variable that tells us whether the customer came to the store during a weekend.

```{r}
train_class %>% 
			mutate(weekend = ifelse((date %>% wday(label = TRUE, abbr = FALSE)) %in% c("Saturday", "Sunday"),TRUE,FALSE)) %>% 
	ggplot(aes(x = as.factor(weekend), fill = target_class == 1)) +
	geom_bar(position = "fill", colour = "black") +
	scale_fill_viridis(discrete = TRUE, name = "Payed?") +
	xlab("Weekend?") +
	# scale_x_discrete(limits = c("FALSE", "TRUE")) +
	ylab("") 

```

We can see that our common sense was somehow right, and the proportion of people that _didn't_ spend any money increases during the weekends.


### channelGrouping
```{r}
train_class %>% 
	group_by(channelGrouping, target_class) %>% 
	summarise(value = n()) %>% 
	ggplot(aes(y = value, x = reorder(channelGrouping, -value))) +
	geom_col(aes(fill = as.factor(target_class))) +
	theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
	scale_fill_viridis(name = "Payed?", discrete = TRUE) +
	xlab("Channel Grouping") +
	ylab("")

train_class %>% 
	ggplot(aes(x = channelGrouping, fill = as.factor(target_class))) +
	geom_bar(position = "fill") +
	theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
	scale_fill_viridis(name = "Payed?", discrete = TRUE) +
	xlab("Channel Grouping") +
	ylab("")
```

The results are quite informative, don't you think? It seems clear that people comming from `Referral` have a greater tendency to end up paying something than the average guy. We could say exactly the opposite for `Social`, since their results are plainly aweful.

The rest are confusing or non-informative. Either by a lack of data or because the distribution is evenly splitted.

### Browser

```{r}
train_class %>% 
	group_by(browser, target_class) %>% 
	summarise(value = n()) %>% 
	ggplot(aes(y = value, x = reorder(browser, -value))) +
	geom_col(aes(fill = as.factor(target_class))) +
	theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
	scale_fill_viridis(name = "Payed?", discrete = TRUE) +
	xlab("Browser") +
	ylab("")

train_class %>% 
	ggplot(aes(x = browser, fill = as.factor(target_class))) +
	geom_bar(position = "fill") +
	theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
	scale_fill_viridis(name = "Payed?", discrete = TRUE) +
	xlab("Browser") +
	ylab("")
```
If I have to be honest, I'd rather drop this feature. Almost everyone is using `Chrome`, which is splitted in the middle and tells us nothing. And the rest are so rare that any meaningful difference could be labeled as an outlier.

If we want to get a bit creative, we could think os simplifying this feature. We could say that from all these browsers we can distinguish `Chrome`, `No-Chrome-but-still-mainstream` and the rest. Let's plot that and see if we can spot something useful.

```{r}
train_class %>% 
	mutate(browser = ifelse(browser %in% c("Safari", "Firefox"), "mainstream", ifelse(browser == "Chrome", 'Chrome', "Other"))) %>% 
	group_by(browser, target_class) %>% 
	summarise(value = n()) %>% 
	ggplot(aes(y = value, x = reorder(browser, -value))) +
	geom_col(aes(fill = as.factor(target_class))) +
	theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
	scale_fill_viridis(name = "Payed?", discrete = TRUE) +
	xlab("Browser") +
	ylab("")

train_class %>% 
	mutate(browser = ifelse(browser %in% c("Safari", "Firefox"), "mainstream", ifelse(browser == "Chrome", 'Chrome', "Other"))) %>% 
	group_by(browser, target_class) %>% 
	ggplot(aes(x = browser, fill = as.factor(target_class))) +
	geom_bar(position = "fill") +
	theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
	scale_fill_viridis(name = "Payed?", discrete = TRUE) +
	xlab("Browser") +
	ylab("")
```

We should remember that _Chrome_ is - hands down - the most used browser both in pc and smartphones. And that after all, we are talking about a Merchandising Google Store. Chances are, that if our customer is not using _Chrome_, she might be not a big fan of Google.

At this point I would just build a feature of type boolean that tracks whether the customer is using `Chrome` or not. And leave it there.

### Source/Medium variable


#### Medium

```{r}
train_class %>% 
	group_by(medium, target_class) %>% 
	summarise(value = n()) %>% 
	ggplot(aes(y = value, x = reorder(medium, -value))) +
	geom_col(aes(fill = as.factor(target_class))) +
	theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
	scale_fill_viridis(name = "Payed?", discrete = TRUE) +
	xlab("Medium") +
	ylab("")

train_class %>% 
	ggplot(aes(x = medium, fill = as.factor(target_class))) +
	geom_bar(position = "fill") +
	theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
	scale_fill_viridis(name = "Payed?", discrete = TRUE) +
	xlab("Medium") +
	ylab("")
```
We could appreciate some slight diffferents based on the medium through which the custimers arrived to the store, and we can see that there are some mediums more effective than others. If I was working in the _Analytics_ team of the GStore, I would recommend them to keep the `referral` system. It leads a lot of traffic, and impressively effective, compared to organic or the others.

#### Source

```{r}
train_class %>% 
	group_by(source, target_class) %>% 
	summarise(value = n()) %>% 
	ungroup() %>% 
	top_n(10, value) %>% 
	ggplot(aes(y = value, x = reorder(source, -value))) +
	geom_col(aes(fill = as.factor(target_class))) +
	theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
	scale_fill_viridis(name = "Payed?", discrete = TRUE) +
	xlab("Source") +
	ylab("")

top_sources <- train_class %>% 
	group_by(source, target_class) %>% 
	summarise(value = n()) %>% 
	ungroup() %>% 
	top_n(10, value) %>% 
	.$source

train_class %>% 
	filter(source %in% top_sources) %>% 
	ggplot(aes(x = source, fill = as.factor(target_class))) +
	geom_bar(position = "fill") +
	theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
	scale_fill_viridis(name = "Payed?", discrete = TRUE) +
	xlab("Source") +
	ylab("")
```


On the other hand, we have the _Source_. Which tells from which place the traffic came from. Note that I limited the plot to the top 10 with more customers, because the list of sources is humongous; and most of them have so small numbers that we couldn't say anything useful from it.

See that the source `mail.googleplex.com` is incredibly efficient. It streams a lot of traffic and almost everyone is paying something. Even though we have the data rebalanced, that is impressive. On the contrary, `youtubecom` seems like a poor source to drag customers from.

#### Source/Medium Variable

In [this](https://www.kaggle.com/erikbruin/google-analytics-eda-lightgbm-screenshots) fantastic Kernel, I saw that the author created a new variable combining both the `Source` and the `Medium`, which could be a significant feature for a model.

```{r}
train_class %>% 
	mutate(source_medium = paste(source, medium, sep="/")) %>% 
	group_by(source_medium, target_class) %>% 
	summarise(value = n()) %>% 
	ungroup() %>% 
	top_n(10, value) %>% 
	ggplot(aes(y = value, x = reorder(source_medium, -value))) +
	geom_col(aes(fill = as.factor(target_class))) +
	theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
	scale_fill_viridis(name = "Payed?", discrete = TRUE) +
	xlab("Source/Medium") +
	ylab("")

top_sourcemediums <- train_class %>% 
	mutate(source_medium = paste(source, medium, sep="/")) %>% 
	group_by(source_medium, target_class) %>% 
	summarise(value = n()) %>% 
	ungroup() %>% 
	top_n(10, value) %>% 
	.$source_medium

train_class %>% 
	mutate(source_medium = paste(source, medium, sep="/")) %>% 
	filter(source_medium %in% top_sourcemediums) %>% 
	ggplot(aes(x = source_medium, fill = as.factor(target_class))) +
	geom_bar(position = "fill") +
	theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
	scale_fill_viridis(name = "Payed?", discrete = TRUE) +
	xlab("Source/Medium") +
	ylab("")
```

However, the results are not much different that those that could be extracted from just the `Source`. Actually, the main important factors that I see here are whether the customer has come from the googleplex mail service, or whether the customer has come from youtube. One is positive and the other is negative, and that is all - the rest might be just for overfitting.

So I would rather not adding this new feature yet.




## Feature Engineering

Based on our EDA, we can fairly construct these features

```{r}
tri <- 1:nrow(train_class)
y <- train_class$target_class %>% as.numeric()

outersect <- function(x, y) {
  sort(c(setdiff(x, y),
         setdiff(y, x)))
}

tmp <- outersect(colnames(train_class),colnames(test_class))


tr_te <- train_class[ , -which(names(train_class) %in% tmp)] %>% 
	rbind(test_class) %>%
	data.table()

rm(train_class, test_class, tmp)
gc()

tr_te[, weekend := ifelse((date %>% wday(label = TRUE, abbr = FALSE)) %in% c("Saturday", "Sunday"),1,0)]
tr_te[, browser := ifelse(browser %in% c("Safari", "Firefox"), "mainstream", ifelse(browser == "Chrome", 'Chrome', "Other"))]
tr_te[, is_chrome_the_browser := ifelse(browser == "Chrome", 1, 0) %>% as.numeric()]
tr_te[, browser := NULL]
tr_te[, source_from_googleplex := ifelse(source == 'mail.googleplex.com', 1, 0) %>% as.numeric()]
tr_te[, source_from_youtube := ifelse(source == 'youtube.com', 1, 0) %>% as.numeric()]
tr_te[, source := NULL]
tr_te[, is_mediu_referral := ifelse(medium == 'referral', 1, 0) %>% as.numeric()]
tr_te[, medium := NULL]

tr_te[,which(unlist(lapply(tr_te, function(x)!all(is.na(x))))),with=F]

tr_te <- tr_te %>% 
	group_by(fullVisitorId, date) %>% 
	mutate_if(is.character, funs(factor(.) %>% as.integer)) %>%
	summarise_all(fn) %>% 
	as_tibble()
```


## XG Boost


```{r}
tri_val <- caret::createDataPartition(y, p = 0.9, list = F) %>% c()
```


Preparing Data
```{r}
trte_xgb <- tr_te %>% 
	ungroup() %>% 
	dplyr::select(-fullVisitorId,-date,-visitId) %>% 
	data.matrix()

#XGB Matrices
dtest_xgb <- xgb.DMatrix(data = trte_xgb[-tri, ])
trte_xgb_xgb <- trte_xgb[tri, ]
dtrain_xgb <- xgb.DMatrix(data = trte_xgb[tri, ], label = y[tri])
dval_xgb <- xgb.DMatrix(data = trte_xgb_xgb[-tri_val, ], label = y[-tri_val])
cols_xgb <- colnames(trte_xgb)

rm(trte_xgb, tri); gc()
```

Training the actual model
```{r}
p <- list(objective = "binary:logistic",
					booster = "gbtree",
					eval_metric = "ndcg",
					nthread = 4,
					eta = 0.001,
					max_depth = 6,
					min_child_weight = 30,
					gamma = 0,
					subsample = 0.85,
					colsample_bytree = 0.7,
					colsample_bylevel = 0.632,
					alpha = 0,
					lambda = 0,
					nrounds = 2000)

m_xgb <- xgb.train(p, dtrain_xgb, p$nrounds, list(val = dval_xgb), print_every_n = 50, early_stopping_rounds = 500)

xgb.importance(cols_xgb, model=m_xgb) %>%
	xgb.plot.importance(top_n = 50)
```


```{r}
probs <- predict(m_xgb, dtest_xgb)
predictions <- (probs > 0.5) %>% as.numeric()

predictions <- predictions %>% 
	as_tibble() %>% 
	rename_(predict_label = names(.)[1]) %>% 
	tibble::rownames_to_column()

correct_pred <- train_class %>% 
	tibble::rownames_to_column() %>% 
	left_join(predictions) %>% 
	filter(predict_label == target_class) 

explainer <- lime(correct_pred, model = m_xgb, 
                  preprocess = as.numeric)
```











```{r}
df <- train %>% 
	group_by(fullVisitorId) %>% 
	summarise(Revenue = (sum(as.numeric(transactionRevenue)))) %>% 
	arrange(desc(Revenue)) %>% 
	mutate(cs_Revenue = cumsum(Revenue)) %>% 
	mutate(ind = row_number()) %>%
	select(ind, cs_Revenue)

df %>% 
	na.omit() %>% 
	ggplot(aes(x = (ind), y = (cs_Revenue))) +
	geom_line(colour = "black", size = 1.2) +
	geom_area(fill = viridis(10)[[5]]) +
	# scale_x_log10() +
	scale_x_continuous(limits = c(0, 3500)) +
	ylab("Accumulated Revenue") +
	xlab("Index of top payers ") +
	theme_minimal()
	
```


